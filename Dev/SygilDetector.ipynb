{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigil Detection\n",
    "\n",
    "**OLD Notebook: the final technique used is in notebook 2**\n",
    "\n",
    "This notebook generates synthetic data and test ideas for sigil detection (detection of the shape the player draws on the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import numba\n",
    "from typing import Any, Callable, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_points = [[0,0], [1,0], [0.5, np.sqrt(1-0.5**2)]]\n",
    "tri_points = np.array(tri_points)\n",
    "\n",
    "center = np.mean(tri_points, axis=0)\n",
    "\n",
    "tri_points = tri_points-center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(5,5))\n",
    "\n",
    "plt.fill(tri_points[:,0], tri_points[:,1], fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def sample_points(vertices:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    points = np.empty((n_point, 2))\n",
    "    n_vertice = vertices.shape[0]\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = np.random.randint(0, n_vertice)\n",
    "        vertice2_index = vertice1_index + 1\n",
    "\n",
    "        if vertice2_index == n_vertice:\n",
    "            vertice2_index = 0\n",
    "\n",
    "        vertice1 = vertices[vertice1_index]\n",
    "        vertice2 = vertices[vertice2_index]\n",
    "\n",
    "        alpha = np.random.rand(1)[0]\n",
    "\n",
    "        points[i] = alpha * vertice1\n",
    "        points[i] += (1-alpha) * vertice2\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def cmp_fn(l, r, *arrays):\n",
    "    for a in numba.literal_unroll(arrays):\n",
    "        if a[l] < a[r]:\n",
    "            return -1  # less than\n",
    "        elif a[l] > a[r]:\n",
    "            return 1   # greater than\n",
    "\n",
    "    return 0  # equal\n",
    "\n",
    "@numba.njit\n",
    "def quicksort(index, L, R, *arrays):\n",
    "    l, r = L, R\n",
    "    pivot = index[(l + r) // 2]\n",
    "\n",
    "    while True:\n",
    "        while l < R and cmp_fn(index[l], pivot, *arrays) == -1:\n",
    "            l += 1\n",
    "        while r >= L and cmp_fn(pivot, index[r], *arrays) == -1:\n",
    "            r -= 1\n",
    "\n",
    "        if l >= r:\n",
    "            break\n",
    "\n",
    "        index[l], index[r] = index[r], index[l]\n",
    "        l += 1\n",
    "        r -= 1\n",
    "\n",
    "        if L < r:\n",
    "            quicksort(index, L, r, *arrays)\n",
    "\n",
    "        if l < R:\n",
    "            quicksort(index, l, R, *arrays)\n",
    "\n",
    "@numba.njit\n",
    "def lexsort(arrays):\n",
    "\n",
    "    if len(arrays) == 0:\n",
    "        return np.empty((), dtype=np.intp)\n",
    "\n",
    "    if len(arrays) == 1:\n",
    "        return np.argsort(arrays[0])\n",
    "\n",
    "\n",
    "    for a in numba.literal_unroll(arrays[1:]):\n",
    "        if a.shape != arrays[0].shape:\n",
    "            raise ValueError(\"lexsort array shapes don't match\")\n",
    "\n",
    "    n = arrays[0].shape[0]\n",
    "    index = np.arange(n)\n",
    "\n",
    "    quicksort(index, 0, n - 1, *arrays)\n",
    "    return index\n",
    "\n",
    "@numba.njit\n",
    "def sample_points_sequential(vertices:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    points = np.empty((n_point, 2))\n",
    "    n_vertice = vertices.shape[0]\n",
    "\n",
    "    info = np.empty((n_point,2))\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = np.random.randint(0, n_vertice)\n",
    "        alpha = np.random.rand(1)[0]\n",
    "\n",
    "        info[i][0] = vertice1_index\n",
    "        info[i][1] = alpha\n",
    "\n",
    "    ind = lexsort((info[:,0], info[:,1]))\n",
    "\n",
    "    info = info[ind]\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = int(info[i][0])\n",
    "        alpha = info[i][1]\n",
    "\n",
    "        vertice2_index = vertice1_index + 1\n",
    "\n",
    "        if vertice2_index == n_vertice:\n",
    "            vertice2_index = 0\n",
    "\n",
    "        vertice1 = vertices[vertice1_index]\n",
    "        vertice2 = vertices[vertice2_index]\n",
    "\n",
    "        points[i] =  (1-alpha) * vertice1\n",
    "        points[i] += alpha * vertice2\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "@numba.njit\n",
    "def rotate_points(points:np.ndarray, angle:float) -> np.ndarray:\n",
    "    angle = np.radians(angle)\n",
    "\n",
    "    rotation_matrix = np.empty((2,2))\n",
    "    rotation_matrix[0,0] = np.cos(angle)\n",
    "    rotation_matrix[0,1] = np.sin(angle)\n",
    "    rotation_matrix[1,0] = -rotation_matrix[0,0]\n",
    "    rotation_matrix[1,1] = rotation_matrix[0,0]\n",
    "\n",
    "    points_r = points @ rotation_matrix\n",
    "\n",
    "    return points_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def add_noise(points, std, add_chance):\n",
    "    n_points = points.shape[0]\n",
    "\n",
    "    noise = np.random.normal(0, std, (n_points, 2))\n",
    "\n",
    "    add_mask_x = np.random.rand(n_points) < add_chance\n",
    "    add_mask_y = np.random.rand(n_points) < add_chance\n",
    "\n",
    "\n",
    "    new_points = points.copy()\n",
    "    new_points[add_mask_x, 0] += noise[add_mask_x, 0]\n",
    "    new_points[add_mask_y, 1] += noise[add_mask_y, 1]\n",
    "\n",
    "    return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def scale_unit(points):\n",
    "    x_min = points[:,0].min()\n",
    "    y_min = points[:,1].min()\n",
    "\n",
    "    x_max = points[:,0].max()\n",
    "    y_max = points[:,1].max()\n",
    "\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "\n",
    "    if x_range > y_range:\n",
    "        scale = 1/x_range\n",
    "    else:\n",
    "        scale = 1/y_range\n",
    "\n",
    "    return points.copy()*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points(tri_points, 100)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "points = rotate_points(points, 45)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "points = add_noise(points, 0.1, 1.0)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "points = scale_unit(points)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_function(points, model_points) -> n_inliers, model\n",
    "def ransac(points, model_function:Callable[[np.ndarray, np.ndarray], Tuple[int, Any]], n, success_rate=0.9, p_inlier=0.8):\n",
    "    k = np.log10(1-success_rate)/np.log10(1-np.power(p_inlier, n))\n",
    "    k = round(k)\n",
    "\n",
    "    n_point = points.shape[0]\n",
    "\n",
    "    best_inliers = -1\n",
    "    best_points = None\n",
    "    best_model = None\n",
    "    for _ in range(k):\n",
    "        indexs = np.random.choice(n_point, size=n, replace=False)\n",
    "        model_points = points[indexs]\n",
    "\n",
    "        n_inliers, model = model_function(points, model_points)\n",
    "\n",
    "        if n_inliers > best_inliers:\n",
    "            best_inliers = n_inliers\n",
    "            best_points = model_points\n",
    "            best_model = model\n",
    "\n",
    "    return best_points, best_inliers, best_model\n",
    "\n",
    "ransac_njit = numba.njit(ransac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@numba.njit\n",
    "def point2line_distance(line_point1, line_point2, point):\n",
    "    #https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line#:~:text=Line%20defined%20by%20two%20points%5Bedit%5D\n",
    "    distance = (line_point2[0]-line_point1[0])*(line_point1[1]-point[1])\n",
    "    distance -= (line_point1[0]-point[0])*(line_point2[1]-line_point1[1])\n",
    "    distance = np.abs(distance)\n",
    "\n",
    "    divider = np.power((line_point2[0]-line_point1[0]), 2)\n",
    "    divider += np.power((line_point2[1]-line_point1[1]), 2)\n",
    "    divider = np.sqrt(divider)\n",
    "\n",
    "    distance /= divider\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def polygon_model(points, model_points, model2vertices_func, inlier_distance) -> Tuple[int, np.ndarray]:\n",
    "    n_inlier = 0\n",
    "\n",
    "    vertices = model2vertices_func(model_points)\n",
    "    n_vertices = vertices.shape[0]\n",
    "\n",
    "    distances = np.empty(n_vertices)\n",
    "\n",
    "    for point in points:\n",
    "\n",
    "        for i in range(n_vertices):\n",
    "            index2 = i+1\n",
    "            if index2 == n_vertices:\n",
    "                index2 = 0\n",
    "\n",
    "            distances[i] = point2line_distance(vertices[i], vertices[index2], point)\n",
    "\n",
    "        if np.min(distances) < inlier_distance:\n",
    "            n_inlier += 1\n",
    "\n",
    "    return n_inlier, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def triangle_model2vertices(model_points):\n",
    "    vertices = np.empty((3,2))\n",
    "    vertices[:2] = model_points\n",
    "\n",
    "    vertices[2] = -np.sum(model_points, 0)\n",
    "\n",
    "    return vertices\n",
    "\n",
    "triangle_inlier_distance = 0.1\n",
    "\n",
    "@numba.njit\n",
    "def triangle_model(points, model_points):\n",
    "    global triangle_inlier_distance\n",
    "    return polygon_model(points, model_points, triangle_model2vertices, triangle_inlier_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_points, n_inliers, model = ransac_njit(points, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "print(n_inliers)\n",
    "\n",
    "vertices = model\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.fill(vertices[:,0], vertices[:,1], fill=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def sample_with_transform(vertices, n_points, max_angle=360, std=0.1, add_chance=1.0, sequential=False):\n",
    "    points = np.empty((1,1))\n",
    "    \n",
    "    if sequential:\n",
    "        points = sample_points_sequential(vertices, n_points)\n",
    "    else:\n",
    "        points = sample_points(vertices, n_points)\n",
    "\n",
    "    angle = max_angle*np.random.rand(1)[0]\n",
    "    points2 = rotate_points(points, angle)\n",
    "\n",
    "    points3 = add_noise(points, 0.1, 1.0)\n",
    "\n",
    "    return points3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_n_inliers_hist = np.empty(1000, int)\n",
    "\n",
    "for i in range(1000):\n",
    "    points = sample_with_transform(tri_points, 100, 120)\n",
    "    points = scale_unit(points)\n",
    "\n",
    "    model_points, n_inliers, model = ransac_njit(points, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "\n",
    "    tri_n_inliers_hist[i] = n_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tri_n_inliers_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_points = [[-1,1], [1,1], [1,-1], [-1,-1]]\n",
    "square_points = np.array(square_points)\n",
    "\n",
    "plt.figure(1, figsize=(5,5))\n",
    "plt.fill(square_points[:,0], square_points[:,1], fill=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def square_model2vertices(model_points, negative_direction):\n",
    "    vertices = np.empty((4,2))\n",
    "    vertices[:2] = model_points\n",
    "\n",
    "    direction1 = np.zeros(3)\n",
    "    direction1[:2] = model_points[1]-model_points[0]\n",
    "\n",
    "    direction2 = np.cross(direction1, [0,0,1])\n",
    "\n",
    "    direction2 = direction2[:2]\n",
    "    direction2 /= np.linalg.norm(direction2)\n",
    "\n",
    "    if negative_direction:\n",
    "        direction2 *= -1\n",
    "\n",
    "    side_len = np.linalg.norm(direction1)\n",
    "    \n",
    "    vertices[2] = model_points[1] + (side_len*direction2)\n",
    "    vertices[3] = model_points[0] + (side_len*direction2)\n",
    "\n",
    "    return vertices\n",
    "\n",
    "@numba.njit\n",
    "def square_model2vertices_p(model_points):\n",
    "    return square_model2vertices(model_points, False)\n",
    "\n",
    "@numba.njit\n",
    "def square_model2vertices_n(model_points):\n",
    "    return square_model2vertices(model_points, True)\n",
    "\n",
    "square_inlier_distance = 0.1\n",
    "\n",
    "@numba.njit\n",
    "def square_model(points, model_points):\n",
    "    n_inlier, vertices = polygon_model(points, model_points, square_model2vertices_p, square_inlier_distance)\n",
    "    n_inlier_n, vertices_n = polygon_model(points, model_points, square_model2vertices_n, square_inlier_distance)\n",
    "\n",
    "    if n_inlier > n_inlier_n:\n",
    "        return n_inlier, vertices\n",
    "    else:\n",
    "        return n_inlier_n, vertices_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_n_inliers_hist = np.empty(1000, int)\n",
    "\n",
    "for i in range(1000):\n",
    "    points = sample_with_transform(square_points, 100, 90)\n",
    "    points = scale_unit(points)\n",
    "\n",
    "    model_points, n_inliers, vertices = ransac_njit(points, square_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "\n",
    "    square_n_inliers_hist[i] = n_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(square_n_inliers_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.fill(vertices[:,0], vertices[:,1], fill=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tri_n_inliers_hist)\n",
    "plt.show()\n",
    "plt.hist(square_n_inliers_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_hist = {}\n",
    "inliers_hist[\"square\"] = {\"square\":[], \"triangle\":[]}\n",
    "inliers_hist[\"triangle\"] = {\"square\":[], \"triangle\":[]}\n",
    "\n",
    "triangle_inlier_distance = 0.1\n",
    "@numba.njit\n",
    "def triangle_model(points, model_points):\n",
    "    global triangle_inlier_distance\n",
    "    return polygon_model(points, model_points, triangle_model2vertices, triangle_inlier_distance)\n",
    "\n",
    "for i in range(1000):\n",
    "    points = sample_with_transform(square_points, 100, 90)\n",
    "    points = scale_unit(points)\n",
    "\n",
    "    model_points, n_inliers, vertices = ransac_njit(points, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "    inliers_hist[\"square\"][\"triangle\"].append(n_inliers)\n",
    "\n",
    "    model_points, n_inliers, vertices = ransac_njit(points, square_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "    inliers_hist[\"square\"][\"square\"].append(n_inliers)\n",
    "\n",
    "for i in range(1000):\n",
    "    points = sample_with_transform(tri_points, 100, 120)\n",
    "    points = scale_unit(points)\n",
    "\n",
    "    model_points, n_inliers, vertices = ransac_njit(points, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "    inliers_hist[\"triangle\"][\"triangle\"].append(n_inliers)\n",
    "\n",
    "    model_points, n_inliers, vertices = ransac_njit(points, square_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "    inliers_hist[\"triangle\"][\"square\"].append(n_inliers)\n",
    "\n",
    "mean_inliers = {}\n",
    "mean_inliers[\"square\"] = {\"square\":0, \"triangle\":0}\n",
    "mean_inliers[\"triangle\"] = {\"square\":0, \"triangle\":0}\n",
    "\n",
    "for key in mean_inliers:\n",
    "    for key2 in mean_inliers[key]:\n",
    "        mean_inliers[key][key2] = np.mean(inliers_hist[key][key2])\n",
    "\n",
    "mean_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points(tri_points, 100)\n",
    "points = rotate_points(points, 90)\n",
    "points = add_noise(points, 0.01, 1.0)\n",
    "\n",
    "model_points, n_inliers, model = ransac_njit(points, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "\n",
    "vertices = triangle_model2vertices(model_points)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.fill(vertices[:,0], vertices[:,1], fill=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ A_i = R B_i$\\\n",
    "$A_i$: sample vertices\\\n",
    "$B_i$: reference triangle vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_points(tri_points, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros((3,3))\n",
    "A = np.zeros((3,3))\n",
    "\n",
    "B[:, :2] = vertices* 10\n",
    "A[:, :2] = tri_points\n",
    "\n",
    "B = B.T\n",
    "A = A.T\n",
    "\n",
    "X = B@A.T\n",
    "\n",
    "u, s, v_t = np.linalg.svd(X)\n",
    "\n",
    "scale = np.eye(3)\n",
    "scale[2,2] = np.linalg.det(u@v_t)\n",
    "\n",
    "R = v_t.T@scale@u.T\n",
    "\n",
    "Rotation.from_matrix(R).as_euler(\"xyz\", degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_euler(\"xyz\", [0, 0, 90] , degrees=True).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_matrix(R).as_euler(\"xyz\", degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_matrix(R).as_euler(\"xyz\", degrees=True)+360-120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triangle = pd.read_csv(\"Points_Triangle.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])\n",
    "df_square = pd.read_csv(\"Points_Square.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points_tr = df_triangle.to_numpy()\n",
    "all_points_sq = df_square.to_numpy()\n",
    "\n",
    "plt.plot(all_points_tr[:,0], all_points_tr[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_points_sq[:,0], all_points_sq[:,1], \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_fromlist(points:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    indexs = np.random.choice(points.shape[0], size=n_point, replace=False)\n",
    "    \n",
    "    return points[indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist(all_points_sq[:200], 100)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_scaled = scale_unit(points)\n",
    "\n",
    "model_points, n_inliers, vertices = ransac_njit(points_scaled, triangle_model, 2, success_rate=0.9, p_inlier=0.2)\n",
    "\n",
    "n_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def center_points(points):\n",
    "    center = np.array([points[:,0].mean(), points[:,1].mean()])\n",
    "    return points.copy()-center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_type = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@numba.njit\n",
    "def create_features(samples_per_type, std_min, std_max):\n",
    "    features = np.empty((samples_per_type*3, 200))\n",
    "    index = 0\n",
    "\n",
    "    for i in range(50000):\n",
    "        std = np.random.uniform(std_min, std_max, 1)[0]\n",
    "        add_chance = np.random.uniform(0.5, 1.0, 1)[0]\n",
    "\n",
    "        points = sample_with_transform(square_points, 100, 90, std, add_chance, True)\n",
    "        points = center_points(points)\n",
    "        points = scale_unit(points)\n",
    "\n",
    "        features[index] = points.flatten()\n",
    "        index += 1\n",
    "\n",
    "    for i in range(50000):\n",
    "        std = np.random.uniform(std_min, std_max, 1)[0]\n",
    "        add_chance = np.random.uniform(0.5, 1.0, 1)[0]\n",
    "\n",
    "        points = sample_with_transform(tri_points, 100, 120, std, add_chance, True)\n",
    "        points = center_points(points)\n",
    "        points = scale_unit(points)\n",
    "\n",
    "        features[index] = points.flatten()\n",
    "        index += 1\n",
    "\n",
    "    for i in range(50000):\n",
    "        points = np.random.rand(100,2)\n",
    "        points = center_points(points)\n",
    "        points = scale_unit(points)\n",
    "\n",
    "        features[index] = points.flatten()\n",
    "        index += 1\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features(10, 1e-3, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.empty(3*samples_per_type, int)\n",
    "targets[:1*samples_per_type] = 0\n",
    "targets[1*samples_per_type:2*samples_per_type] = 1\n",
    "targets[2*samples_per_type:] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 3*samples_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.choice(total_samples, size=total_samples, replace=False)\n",
    "\n",
    "train_features = features[indexes[:int(0.7*total_samples)]]\n",
    "train_targets = targets[indexes[:int(0.7*total_samples)]]\n",
    "\n",
    "val_features = features[indexes[int(0.7*total_samples):int(0.9*total_samples)]]\n",
    "val_targets = targets[indexes[int(0.7*total_samples):int(0.9*total_samples)]]\n",
    "\n",
    "test_features = features[indexes[int(0.9*total_samples):]]\n",
    "test_targets = targets[indexes[int(0.9*total_samples):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\"train_features\": train_features,\n",
    "\"train_targets\": train_targets,\n",
    "\"val_features\": val_features,\n",
    "\"val_targets\": val_targets,\n",
    "\"test_features\": test_features,\n",
    "\"test_targets\": test_targets,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data3.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data3.npy\", allow_pickle=True).item()\n",
    "train_features = data[\"train_features\"]\n",
    "train_targets = data[\"train_targets\"]\n",
    "val_features = data[\"val_features\"]\n",
    "val_targets = data[\"val_targets\"]\n",
    "test_features = data[\"test_features\"]\n",
    "test_targets = data[\"test_targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100*2\n",
    "\n",
    "n_train = train_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.argwhere(train_targets == 1)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    a = train_features[indexes[i]].reshape((100,2))\n",
    "    plt.plot(a[:,0], a[:,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(model, epochs, optimizer, batch_size, patience, units, regularizer, others=\"\"):\n",
    "    learning_rate = optimizer.learning_rate.numpy()\n",
    "    \n",
    "    config = {}\n",
    "    config[\"leaning_rate\"] = learning_rate\n",
    "    config[\"epochs\"] = epochs\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"optimizer\"] = optimizer._name\n",
    "    config[\"others\"] = others\n",
    "\n",
    "    config[\"layer0_units\"] = units[0]\n",
    "    config[\"layer0_l2\"] = regularizer[0]\n",
    "\n",
    "    if len(units) > 1:\n",
    "        config[\"layer1_units\"] = units[1]\n",
    "        config[\"layer1_l2\"] = regularizer[1]\n",
    "    else:\n",
    "        config[\"layer1_units\"] = 0\n",
    "        config[\"layer1_l2\"] = 0\n",
    "\n",
    "    run = wandb.init(project=\"SygilDetector\", config = config)\n",
    "\n",
    "\n",
    "    callbacks = []\n",
    "    callbacks.append(keras.callbacks.EarlyStopping(monitor=\"val_sparse_categorical_crossentropy\", patience=patience))\n",
    "    callbacks.append(WandbCallback(\"val_sparse_categorical_crossentropy\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_crossentropy\", \"accuracy\"])\n",
    "\n",
    "    hist = model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=epochs, batch_size=batch_size,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [400, 400]\n",
    "regularizer = [0.0075, 0.0075]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Input(shape=(n_features), name=\"input\"),\n",
    "                    keras.layers.Dense(units[0], activation=\"relu\", kernel_regularizer=l2(regularizer[0])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(units[1], activation=\"relu\", kernel_regularizer=l2(regularizer[1])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(3, activation=\"softmax\")])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)#(learning_rate=0.0005)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "compile_fit(model, 1000, optimizer, 10000//2, 50, units, regularizer, \"dropout[0.5, 0.5]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_sq = sample_points_fromlist(all_points_sq, 100)\n",
    "points_tr = sample_points_fromlist(all_points_tr, 100)\n",
    "\n",
    "points_sq = center_points(points_sq)\n",
    "points_tr = center_points(points_tr)\n",
    "\n",
    "points_sq = scale_unit(points_sq)\n",
    "points_tr = scale_unit(points_tr)\n",
    "\n",
    "result = model(np.array([points_sq.flatten(), points_tr.flatten()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(result.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "onnx_model = tf2onnx.convert.from_keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2onnx.utils.save_onnx_model(\".\", \"sygil_detector.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"sygil_detector.onnx\"), \"wb\") as f:\n",
    "    f.write(onnx_model[0].SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triangle = pd.read_csv(\"Points_Triangle2.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])\n",
    "all_points_tr = df_triangle.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist(all_points_tr, 100)\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(np.array([points.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist(all_points_sq, 100)\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(np.array([points.flatten()])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])\n",
    "points = df.to_numpy()[100:]\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model(np.array([points.flatten()])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(np.array([points.flatten()/2])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[:,0]/2, points[:,1]/2, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_fromlist_sequential(points:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    indexs = np.random.choice(points.shape[0], size=n_point, replace=False)\n",
    "\n",
    "    indexs = np.sort(indexs)\n",
    "    \n",
    "    return points[indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist_sequential(all_points_tr, 100)\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)\n",
    "\n",
    "model(np.array([points.flatten()])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evolvepy as ep\n",
    "\n",
    "\n",
    "def create_generator(model):\n",
    "    first = ep.generator.Layer()\n",
    "    combine = ep.generator.CombineLayer(ep.generator.selection.tournament, ep.generator.crossover.one_point)\n",
    "    mutation = ep.generator.mutation.NumericMutationLayer(ep.generator.mutation.sum_mutation, 1.0, 0.5, (-0.5, 0.5))\n",
    "    filter0 = ep.generator.FilterFirsts(143)\n",
    "    sort = ep.generator.Sort()\n",
    "    filter1 = ep.generator.FilterFirsts(7)\n",
    "    concat = ep.generator.Concatenate()\n",
    "\n",
    "    first.next = combine\n",
    "    combine.next = mutation\n",
    "    combine.next = filter0\n",
    "    filter0.next = concat\n",
    "\n",
    "    first.next = sort\n",
    "    sort.next = filter1\n",
    "    filter1.next = concat\n",
    "\n",
    "    #Creates the generator using the descriptor.\n",
    "    generator = ep.generator.Generator(first_layer=first, last_layer=concat, descriptor=model.descriptor)\n",
    "\n",
    "    return generator, mutation.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit_ep(model, epochs, generator, batch_size, patience, units, regularizer, others=\"\"):\n",
    "    \n",
    "    config = {}\n",
    "    config[\"epochs\"] = epochs\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"optimizer\"] = \"evolutionary\"\n",
    "    config[\"others\"] = others\n",
    "\n",
    "    config[\"layer0_units\"] = units[0]\n",
    "    config[\"layer0_l2\"] = regularizer[0]\n",
    "\n",
    "    if len(units) > 1:\n",
    "        config[\"layer1_units\"] = units[1]\n",
    "        config[\"layer1_l2\"] = regularizer[1]\n",
    "    else:\n",
    "        config[\"layer1_units\"] = 0\n",
    "        config[\"layer1_l2\"] = 0\n",
    "\n",
    "    run = wandb.init(project=\"SygilDetector\", config = config)\n",
    "\n",
    "    callbacks = []\n",
    "    #callbacks.append(keras.callbacks.EarlyStopping(monitor=\"val_sparse_categorical_crossentropy\", patience=patience))\n",
    "    callbacks.append(WandbCallback(\"val_sparse_categorical_crossentropy\"))\n",
    "\n",
    "    model.compile(generator, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_crossentropy\", \"accuracy\"])\n",
    "\n",
    "    hist = model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=epochs, batch_size=batch_size,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evolvepy.integrations.tf_keras import EvolutionaryModel\n",
    "\n",
    "units = [400, 400]\n",
    "regularizer = [0, 0]\n",
    "\n",
    "model = EvolutionaryModel([keras.layers.Input(shape=(n_features), name=\"input\"),\n",
    "                    keras.layers.Dense(units[0], activation=\"relu\"),\n",
    "                    keras.layers.Dense(units[1], activation=\"relu\"),\n",
    "                    keras.layers.Dense(3, activation=\"softmax\")])\n",
    "                    \n",
    "generator, _ = create_generator(model)\n",
    "\n",
    "model.compile(generator, 150, loss=\"sparse_categorical_crossentropy\",  metrics=[\"sparse_categorical_crossentropy\", \"accuracy\"])\n",
    "\n",
    "hist = compile_fit_ep(model, 400, generator, 10000, 50, units, regularizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
