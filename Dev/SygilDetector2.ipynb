{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigil Detection\n",
    "\n",
    "This notebook generates synthetic data and trains models for sigil detection (detection of the shape the player draws on the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import numba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "import tf2onnx\n",
    "from scipy.spatial.transform import Rotation\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample points between vertices. Random and random-sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def sample_points(vertices:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    points = np.empty((n_point, 2))\n",
    "    n_vertice = vertices.shape[0]\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = np.random.randint(0, n_vertice)\n",
    "        vertice2_index = vertice1_index + 1\n",
    "\n",
    "        if vertice2_index == n_vertice:\n",
    "            vertice2_index = 0\n",
    "\n",
    "        vertice1 = vertices[vertice1_index]\n",
    "        vertice2 = vertices[vertice2_index]\n",
    "\n",
    "        alpha = np.random.rand(1)[0]\n",
    "\n",
    "        points[i] = alpha * vertice1\n",
    "        points[i] += (1-alpha) * vertice2\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def cmp_fn(l, r, *arrays):\n",
    "    for a in numba.literal_unroll(arrays):\n",
    "        if a[l] < a[r]:\n",
    "            return -1  # less than\n",
    "        elif a[l] > a[r]:\n",
    "            return 1   # greater than\n",
    "\n",
    "    return 0  # equal\n",
    "\n",
    "@numba.njit\n",
    "def quicksort(index, L, R, *arrays):\n",
    "    l, r = L, R\n",
    "    pivot = index[(l + r) // 2]\n",
    "\n",
    "    while True:\n",
    "        while l < R and cmp_fn(index[l], pivot, *arrays) == -1:\n",
    "            l += 1\n",
    "        while r >= L and cmp_fn(pivot, index[r], *arrays) == -1:\n",
    "            r -= 1\n",
    "\n",
    "        if l >= r:\n",
    "            break\n",
    "\n",
    "        index[l], index[r] = index[r], index[l]\n",
    "        l += 1\n",
    "        r -= 1\n",
    "\n",
    "        if L < r:\n",
    "            quicksort(index, L, r, *arrays)\n",
    "\n",
    "        if l < R:\n",
    "            quicksort(index, l, R, *arrays)\n",
    "\n",
    "@numba.njit\n",
    "def lexsort(arrays):\n",
    "\n",
    "    if len(arrays) == 0:\n",
    "        return np.empty((), dtype=np.intp)\n",
    "\n",
    "    if len(arrays) == 1:\n",
    "        return np.argsort(arrays[0])\n",
    "\n",
    "\n",
    "    for a in numba.literal_unroll(arrays[1:]):\n",
    "        if a.shape != arrays[0].shape:\n",
    "            raise ValueError(\"lexsort array shapes don't match\")\n",
    "\n",
    "    n = arrays[0].shape[0]\n",
    "    index = np.arange(n)\n",
    "\n",
    "    quicksort(index, 0, n - 1, *arrays)\n",
    "    return index\n",
    "\n",
    "@numba.njit\n",
    "def sample_points_sequential(vertices:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    points = np.empty((n_point, 2))\n",
    "    n_vertice = vertices.shape[0]\n",
    "\n",
    "    info = np.empty((n_point,2))\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = np.random.randint(0, n_vertice)\n",
    "        alpha = np.random.rand(1)[0]\n",
    "\n",
    "        info[i][0] = vertice1_index\n",
    "        info[i][1] = alpha\n",
    "\n",
    "    ind = lexsort((info[:,0], info[:,1]))\n",
    "\n",
    "    info = info[ind]\n",
    "\n",
    "    for i in range(n_point):\n",
    "        vertice1_index = int(info[i][0])\n",
    "        alpha = info[i][1]\n",
    "\n",
    "        vertice2_index = vertice1_index + 1\n",
    "\n",
    "        if vertice2_index == n_vertice:\n",
    "            vertice2_index = 0\n",
    "\n",
    "        vertice1 = vertices[vertice1_index]\n",
    "        vertice2 = vertices[vertice2_index]\n",
    "\n",
    "        points[i] =  (1-alpha) * vertice1\n",
    "        points[i] += alpha * vertice2\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def create_rotation_matrix(angle):\n",
    "    angle = np.radians(angle)\n",
    "\n",
    "    rotation_matrix = np.empty((2,2))\n",
    "    rotation_matrix[0,0] = np.cos(angle)\n",
    "    rotation_matrix[0,1] = -np.sin(angle)\n",
    "    rotation_matrix[1,0] = -rotation_matrix[0,1]\n",
    "    rotation_matrix[1,1] = rotation_matrix[0,0]\n",
    "\n",
    "    return rotation_matrix\n",
    "\n",
    "@numba.njit\n",
    "def rotate_points(points:np.ndarray, angle:float) -> np.ndarray:\n",
    "\n",
    "    points_r = points @ create_rotation_matrix(angle)\n",
    "\n",
    "    return points_r\n",
    "\n",
    "@numba.njit\n",
    "def add_noise(points, std, add_chance, max_noise):\n",
    "    n_points = points.shape[0]\n",
    "\n",
    "    noise = np.random.normal(0, std, (n_points, 2))\n",
    "    noise = np.clip(noise, -max_noise, max_noise)\n",
    "\n",
    "    add_mask_x = np.random.rand(n_points) < add_chance\n",
    "    add_mask_y = np.random.rand(n_points) < add_chance\n",
    "\n",
    "    new_points = points.copy()\n",
    "    new_points[add_mask_x, 0] += noise[add_mask_x, 0]\n",
    "    new_points[add_mask_y, 1] += noise[add_mask_y, 1]\n",
    "\n",
    "    return new_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def sample_with_transform(vertices, n_points, max_angle=360, std=0.1, add_chance=1.0, max_noise=0.05, sequential=False):\n",
    "    points = np.empty((1,1))\n",
    "    \n",
    "    if sequential:\n",
    "        points = sample_points_sequential(vertices, n_points)\n",
    "    else:\n",
    "        points = sample_points(vertices, n_points)\n",
    "\n",
    "    angle = max_angle*np.random.rand(1)[0]\n",
    "    points = rotate_points(points, angle)\n",
    "\n",
    "    points = add_noise(points, std, add_chance, max_noise)\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def center_points(points):\n",
    "    center = np.array([points[:,0].mean(), points[:,1].mean()])\n",
    "    return points.copy()-center\n",
    "\n",
    "@numba.njit\n",
    "def scale_unit(points):\n",
    "    x_min = points[:,0].min()\n",
    "    y_min = points[:,1].min()\n",
    "\n",
    "    x_max = points[:,0].max()\n",
    "    y_max = points[:,1].max()\n",
    "\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "\n",
    "    if x_range > y_range:\n",
    "        scale = 1/x_range\n",
    "    else:\n",
    "        scale = 1/y_range\n",
    "\n",
    "    return points.copy()*scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def generate_regular_vertices(n_side):\n",
    "    angle_increment = 360/n_side\n",
    "\n",
    "    angle = 0\n",
    "\n",
    "    base_point = np.array([1,0], np.float64)\n",
    "    vertices = np.empty((n_side, 2), np.float64)\n",
    "\n",
    "    for i in range(n_side):\n",
    "        rotation_matrix = create_rotation_matrix(angle)\n",
    "\n",
    "        vertices[i] = rotation_matrix@base_point\n",
    "\n",
    "        angle += angle_increment\n",
    "    \n",
    "    return vertices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def generate_random_vertices(n_side):\n",
    "    return np.random.rand(n_side, 2)\n",
    "\n",
    "@numba.njit\n",
    "def generate_random_vertices2():\n",
    "    angle = 0\n",
    "    angles = []\n",
    "\n",
    "    while angle < 360:\n",
    "        angles.append(angle)\n",
    "        angle += 10*np.random.rand(1)[0]\n",
    "    \n",
    "    vertices = np.empty((len(angles), 2), np.float64)\n",
    "    base_point = np.array([1,0], np.float64)\n",
    "\n",
    "    for i in range(len(angles)):\n",
    "        rotation_matrix = create_rotation_matrix(angles[i])\n",
    "        vertices[i] = rotation_matrix@base_point\n",
    "\n",
    "    return vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_min = 1e-4\n",
    "add_chance_min = 0.2\n",
    "\n",
    "max_noise = 1e-1\n",
    "std_max = 5e-2\n",
    "add_chance_max = 1.0\n",
    "\n",
    "@numba.njit\n",
    "def create_dataset(samples_per_type, n_sides, random_vertices_min, random_vertices_max):\n",
    "    n_sides = np.asarray(n_sides)\n",
    "\n",
    "    n_type = len(n_sides) + 2\n",
    "\n",
    "    features = np.empty((samples_per_type*n_type, 200))\n",
    "    labels = np.empty(samples_per_type*n_type, np.int32)\n",
    "    index = 0\n",
    "\n",
    "    for i in range(len(n_sides)):\n",
    "        vertices = generate_regular_vertices(n_sides[i])\n",
    "\n",
    "        for _ in range(samples_per_type):\n",
    "            std = np.random.uniform(std_min, std_max, 1)[0]\n",
    "            add_chance = np.random.uniform(add_chance_min, add_chance_max, 1)[0]\n",
    "\n",
    "            points = sample_with_transform(vertices, 100, 360, std, add_chance, max_noise, True)\n",
    "            points = center_points(points)\n",
    "            points = scale_unit(points)\n",
    "\n",
    "            features[index] = points.flatten()\n",
    "            labels[index] = i\n",
    "            index += 1\n",
    "\n",
    "    for _ in range(2*samples_per_type):\n",
    "        std = np.random.uniform(std_min, std_max, 1)[0]\n",
    "        add_chance = np.random.uniform(add_chance_min, add_chance_max, 1)[0]\n",
    "        n_vertice = np.random.randint(random_vertices_min, random_vertices_max)\n",
    "    \n",
    "        vertices = generate_random_vertices(n_vertice)\n",
    "    \n",
    "        points = sample_with_transform(vertices, 100, 360, std, add_chance, max_noise, True)\n",
    "        points = center_points(points)\n",
    "        points = scale_unit(points)\n",
    "    \n",
    "        features[index] = points.flatten()\n",
    "        labels[index] = len(n_sides)\n",
    "        index += 1\n",
    "\n",
    "    #for _ in range(samples_per_type):\n",
    "    #    std = np.random.uniform(std_min, std_max, 1)[0]\n",
    "    #    add_chance = np.random.uniform(add_chance_min, add_chance_max, 1)[0]\n",
    "    #\n",
    "    #    vertices = generate_random_vertices2()\n",
    "    #\n",
    "    #    points = sample_with_transform(vertices, 100, 360, std, add_chance, max_noise, True)\n",
    "    #    points = center_points(points)\n",
    "    #    points = scale_unit(points)\n",
    "    #\n",
    "    #    features[index] = points.flatten()\n",
    "    #    labels[index] = len(n_sides)+1\n",
    "    #    index += 1\n",
    "\n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_type = 5000\n",
    "\n",
    "features, labels = create_dataset(samples_per_type, np.array([3, 4, 5, 6]), 6, 12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = features.shape[0]\n",
    "train_percentage = 0.7\n",
    "val_percentage = 0.2\n",
    "test_percentage = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.choice(total_samples, size=total_samples, replace=False)\n",
    "\n",
    "val_start = train_percentage+val_percentage\n",
    "\n",
    "train_features = features[indexes[:int(train_percentage*total_samples)]]\n",
    "train_targets = labels[indexes[:int(train_percentage*total_samples)]]\n",
    "\n",
    "val_features = features[indexes[int(train_percentage*total_samples):int(val_start*total_samples)]]\n",
    "val_targets = labels[indexes[int(train_percentage*total_samples):int(val_start*total_samples)]]\n",
    "\n",
    "test_features = features[indexes[int(val_start*total_samples):]]\n",
    "test_targets = labels[indexes[int(val_start*total_samples):]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\"train_features\": train_features,\n",
    "\"train_targets\": train_targets,\n",
    "\"val_features\": val_features,\n",
    "\"val_targets\": val_targets,\n",
    "\"test_features\": test_features,\n",
    "\"test_targets\": test_targets,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data_sygil4.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data_sygil2.npy\", allow_pickle=True).item()\n",
    "train_features = data[\"train_features\"]\n",
    "train_targets = data[\"train_targets\"]\n",
    "val_features = data[\"val_features\"]\n",
    "val_targets = data[\"val_targets\"]\n",
    "test_features = data[\"test_features\"]\n",
    "test_targets = data[\"test_targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100*2\n",
    "n_train = train_features.shape[0]\n",
    "n_class = np.max([train_targets.max(), val_targets.max(), test_targets.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(model, epochs, optimizer, batch_size, patience, units, regularizer, others=\"\"):\n",
    "    learning_rate = optimizer.learning_rate.numpy()\n",
    "    \n",
    "    config = {}\n",
    "    config[\"leaning_rate\"] = learning_rate\n",
    "    config[\"epochs\"] = epochs\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"optimizer\"] = optimizer._name\n",
    "    config[\"others\"] = others\n",
    "\n",
    "    config[\"layer0_units\"] = units[0]\n",
    "    config[\"layer0_l2\"] = regularizer[0]\n",
    "\n",
    "    if len(units) > 1:\n",
    "        config[\"layer1_units\"] = units[1]\n",
    "        config[\"layer1_l2\"] = regularizer[1]\n",
    "    else:\n",
    "        config[\"layer1_units\"] = 0\n",
    "        config[\"layer1_l2\"] = 0\n",
    "\n",
    "    run = wandb.init(project=\"SygilDetector\", config = config)\n",
    "\n",
    "\n",
    "    callbacks = []\n",
    "    callbacks.append(keras.callbacks.EarlyStopping(monitor=\"val_sparse_categorical_crossentropy\", patience=patience))\n",
    "    callbacks.append(WandbCallback(\"val_sparse_categorical_crossentropy\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_crossentropy\", \"accuracy\"])\n",
    "\n",
    "    hist = model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=epochs, batch_size=batch_size,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [400, 400]\n",
    "regularizer = [0.0075, 0.0075]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Input(shape=(n_features), name=\"input\"),\n",
    "                    keras.layers.Dense(units[0], activation=\"relu\", kernel_regularizer=l2(regularizer[0])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(units[1], activation=\"relu\", kernel_regularizer=l2(regularizer[1])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(n_class, activation=\"softmax\")])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)#(learning_rate=0.0005)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "hist = compile_fit(model, 1000, optimizer, 10000//2, 50, units, regularizer, \"dropout[0.5, 0.5]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history[\"loss\"])\n",
    "plt.plot(model.history.history[\"val_loss\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model.history.history[\"accuracy\"])\n",
    "plt.plot(model.history.history[\"val_accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_fromlist(points:np.ndarray, n_point:int) -> np.ndarray:\n",
    "    indexes = np.random.choice(points.shape[0], size=n_point, replace=False)\n",
    "    indexes = np.sort(indexes)\n",
    "\n",
    "\n",
    "    return points[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triangle = pd.read_csv(\"Points_Triangle.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])\n",
    "all_points_tr = df_triangle.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist(all_points_tr, 100)\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)\n",
    "\n",
    "result = model(np.array([points.flatten()])).numpy()[0]\n",
    "\n",
    "print(result)\n",
    "print(np.argmax(result[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "plt.fill(points[:,0], points[:,1], fill=False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input.csv\", sep=\";\", decimal=\",\", names=[\"x\", \"y\"])\n",
    "points = df.to_numpy()[100:]\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)\n",
    "\n",
    "result = model(np.array([points.flatten()])).numpy()[0]\n",
    "\n",
    "print(result)\n",
    "print(np.argmax(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[:,0], points[:,1], \"o\")\n",
    "plt.show()\n",
    "\n",
    "plt.fill(points[:,0], points[:,1], fill=False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    points = train_features[i].reshape(100,2)\n",
    "\n",
    "    img = np.zeros((img_size,img_size))\n",
    "    for point in points:\n",
    "        index = (point+1)/2\n",
    "        index *= img_size\n",
    "        index = index.astype(np.int32)\n",
    "\n",
    "        img[index[0], index[1]] = 1\n",
    "\n",
    "    print(train_targets[i])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points2image(features):\n",
    "    n_data = features.shape[0]\n",
    "\n",
    "    img_features = np.empty((n_data, img_size, img_size, 1), np.uint8)\n",
    "\n",
    "    for i in range(n_data):\n",
    "        points = features[i].reshape(100,2)\n",
    "\n",
    "        img = np.zeros((img_size,img_size))\n",
    "        for point in points:\n",
    "            index = (point+1)/2\n",
    "            index *= img_size\n",
    "            index = index.astype(np.int32)\n",
    "\n",
    "            img_features[i, index[0], index[1], 0] = 255\n",
    "\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = points2image(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = img_features.shape[0]\n",
    "train_percentage = 0.7\n",
    "val_percentage = 0.2\n",
    "test_percentage = 0.1\n",
    "\n",
    "indexes = np.random.choice(total_samples, size=total_samples, replace=False)\n",
    "\n",
    "val_start = train_percentage+val_percentage\n",
    "\n",
    "train_features = img_features[indexes[:int(train_percentage*total_samples)]]\n",
    "train_targets = labels[indexes[:int(train_percentage*total_samples)]]\n",
    "\n",
    "val_features = img_features[indexes[int(train_percentage*total_samples):int(val_start*total_samples)]]\n",
    "val_targets = labels[indexes[int(train_percentage*total_samples):int(val_start*total_samples)]]\n",
    "\n",
    "test_features = img_features[indexes[int(val_start*total_samples):]]\n",
    "test_targets = labels[indexes[int(val_start*total_samples):]]\n",
    "\n",
    "data = {\n",
    "\"train_features\": train_features,\n",
    "\"train_targets\": train_targets,\n",
    "\"val_features\": val_features,\n",
    "\"val_targets\": val_targets,\n",
    "\"test_features\": test_features,\n",
    "\"test_targets\": test_targets,\n",
    "}\n",
    "\n",
    "np.save(\"data_sygil_img.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data_sygil_img.npy\", allow_pickle=True).item()\n",
    "train_features = data[\"train_features\"]\n",
    "train_targets = data[\"train_targets\"]\n",
    "val_features = data[\"val_features\"]\n",
    "val_targets = data[\"val_targets\"]\n",
    "test_features = data[\"test_features\"]\n",
    "test_targets = data[\"test_targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_features.shape[0]\n",
    "n_class = np.max([train_targets.max(), val_targets.max(), test_targets.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [400, 400]\n",
    "regularizer = [0.0075, 0.0075]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Input(shape=(img_size, img_size, 1), name=\"input\"),\n",
    "                    keras.layers.Flatten(),\n",
    "                    keras.layers.Dense(units[0], activation=\"relu\", kernel_regularizer=l2(regularizer[0])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(units[1], activation=\"relu\", kernel_regularizer=l2(regularizer[1])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(n_class, activation=\"softmax\")])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)#(learning_rate=0.0005)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "hist = compile_fit(model, 1000, optimizer, 10000//2, 50, units, regularizer, \"dropout[0.5, 0.5]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample_points_fromlist(all_points_tr, 100)\n",
    "points = center_points(points)\n",
    "points = scale_unit(points)\n",
    "\n",
    "img = points2image(np.array([points]))\n",
    "\n",
    "print(model(img).numpy()[0])\n",
    "np.argmax(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(np.array([points.flatten()])).numpy()[0]\n",
    "\n",
    "print(result)\n",
    "print(np.argmax(result[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit_cnn(model, epochs, optimizer, batch_size, patience, units, regularizer, others=\"\"):\n",
    "    learning_rate = optimizer.learning_rate.numpy()\n",
    "    \n",
    "    config = {}\n",
    "    config[\"leaning_rate\"] = learning_rate\n",
    "    config[\"epochs\"] = epochs\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"optimizer\"] = optimizer._name\n",
    "    config[\"others\"] = others\n",
    "\n",
    "    run = wandb.init(project=\"SygilDetector\", config = config)\n",
    "\n",
    "    callbacks = []\n",
    "    callbacks.append(keras.callbacks.EarlyStopping(monitor=\"val_sparse_categorical_crossentropy\", patience=patience))\n",
    "    callbacks.append(WandbCallback(\"val_sparse_categorical_crossentropy\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_crossentropy\", \"accuracy\"])\n",
    "\n",
    "    hist = model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=epochs, batch_size=batch_size,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [400, 400]\n",
    "regularizer = [0.0075, 0.0075]\n",
    "\n",
    "#model = keras.Sequential([keras.layers.Input(shape=(img_size, img_size, 1), name=\"input\"),\n",
    "#                    keras.layers.Conv2D(16, (3,3), activation=\"relu\", padding=\"same\", kernel_regularizer=l2(regularizer[0])),\n",
    "#                    keras.layers.MaxPooling2D((3,3)),\n",
    "#                    keras.layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\", kernel_regularizer=l2(regularizer[0])),\n",
    "#                    keras.layers.MaxPooling2D((3,3)),\n",
    "#                    keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=l2(regularizer[1])),\n",
    "#                    keras.layers.Dropout(0.5),\n",
    "#                    keras.layers.Dense(n_class, activation=\"softmax\")])\n",
    "\n",
    "model = keras.Sequential([keras.layers.Input(shape=(img_size, img_size, 1), name=\"input\"),\n",
    "                    keras.layers.Conv2D(16, (3,3), activation=\"relu\", padding=\"same\", kernel_regularizer=l2(regularizer[0])),\n",
    "                    keras.layers.MaxPooling2D((3,3)),\n",
    "                    keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=l2(regularizer[1])),\n",
    "                    keras.layers.Dropout(0.5),\n",
    "                    keras.layers.Dense(n_class, activation=\"softmax\")])\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)#(learning_rate=0.0005)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "hist = compile_fit_cnn(model, 1000, optimizer, 10000//2, 50, units, regularizer, \"dropout[0.5, 0.5]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = tf2onnx.convert.from_keras(model)\n",
    "\n",
    "with open(os.path.join(\"sygil_detector_img1.onnx\"), \"wb\") as f:\n",
    "    f.write(onnx_model[0].SerializeToString())\n",
    "\n",
    "model.save(\"sygil_detector_img1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
